import { type GenerateTextParams, ModelType, logger } from '@elizaos/core';

/**
 * Interface representing the structure of an Ollama model.
 * @typedef {Object} OllamaModel
 * @property {string} name - The name of the Ollama model.
 * @property {string} id - The unique identifier of the Ollama model.
 * @property {string} size - The size of the Ollama model.
 * @property {string} modified - The date when the Ollama model was last modified.
 */
interface OllamaModel {
  name: string;
  id: string;
  size: string;
  modified: string;
}

/**
 * Interface representing a response from the Ollama API.
 * @property {string} model - The model used for generating the response.
 * @property {string} response - The actual response generated by the model.
 * @property {boolean} done - Indicates whether the response generation is complete.
 * @property {number[]} [prompt] - Optional array of prompt values used in generating the response.
 * @property {number} [total_duration] - Optional total duration of the response generation process.
 * @property {number} [load_duration] - Optional load duration of the model used.
 * @property {number} [prompt_eval_duration] - Optional evaluation duration for the prompt values.
 * @property {number} [eval_duration] - Optional evaluation duration for the response generation.
 */
interface OllamaResponse {
  model: string;
  response: string;
  done: boolean;
  prompt?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_duration?: number;
  eval_duration?: number;
}

/**
 * Manages interactions with the Ollama API, including server status checks, fetching available models,
 * testing models, initializing the manager, generating text, and more.
 */
export class OllamaManager {
  private static instance: OllamaManager | null = null;
  private serverUrl: string;
  private initialized = false;
  private availableModels: OllamaModel[] = [];
  private configuredModels = {
    small: process.env.SMALL_OLLAMA_MODEL || 'deepseek-r1:1.5b',
    medium: process.env.MEDIUM_OLLAMA_MODEL || 'deepseek-r1:7b',
  };

  /**
   * Private constructor for initializing OllamaManager.
   */
  private constructor() {
    this.serverUrl = process.env.OLLAMA_SERVER_URL || 'http://192.168.1.90:11434';
    logger.info('OllamaManager initialized with configuration:', {
      serverUrl: this.serverUrl,
      configuredModels: this.configuredModels,
      timestamp: new Date().toISOString(),
    });
  }

  /**
   * Returns an instance of the OllamaManager class.
   * If an instance does not already exist, a new instance is created and returned.
   * @returns {OllamaManager} The instance of the OllamaManager class.
   */
  public static getInstance(): OllamaManager {
    if (!OllamaManager.instance) {
      OllamaManager.instance = new OllamaManager();
    }
    return OllamaManager.instance;
  }

  /**
   * Asynchronously checks the status of the server by attempting to fetch the "/api/tags" endpoint.
   * @returns A Promise that resolves to a boolean indicating if the server is reachable and responding with a successful status.
   */
  private async checkServerStatus(): Promise<boolean> {
    try {
      const response = await fetch(`${this.serverUrl}/api/tags`);
      if (!response.ok) {
        throw new Error(`Server responded with status: ${response.status}`);
      }
      return true;
    } catch (error) {
      logger.error('Ollama server check failed:', {
        error: error instanceof Error ? error.message : String(error),
        serverUrl: this.serverUrl,
        timestamp: new Date().toISOString(),
      });
      return false;
    }
  }

  /**
   * Fetches the available Ollama models from the specified server URL.
   *
   * @returns {Promise<void>} A Promise that resolves when the available models are successfully fetched.
   */
  private async fetchAvailableModels(): Promise<void> {
    try {
      const response = await fetch(`${this.serverUrl}/api/tags`);
      if (!response.ok) {
        throw new Error(`Failed to fetch models: ${response.status}`);
      }

      const data = (await response.json()) as { models: OllamaModel[] };
      this.availableModels = data.models;

      logger.info('Ollama available models:', {
        count: this.availableModels.length,
        models: this.availableModels.map((m) => m.name),
        timestamp: new Date().toISOString(),
      });
    } catch (error) {
      logger.error('Failed to fetch Ollama models:', {
        error: error instanceof Error ? error.message : String(error),
        serverUrl: this.serverUrl,
        timestamp: new Date().toISOString(),
      });
      throw error;
    }
  }

  /**
   * Asynchronously tests a model specified by the given modelId.
   *
   * @param {string} modelId - The ID of the model to be tested.
   * @returns {Promise<boolean>} - A promise that resolves to true if the model test is successful, false otherwise.
   */
  private async testModel(modelId: string): Promise<boolean> {
    try {
      const testRequest = {
        model: modelId,
        prompt:
          "Debug Mode: Test initialization. Respond with 'Initialization successful' if you can read this.",
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 100,
        },
      };

      logger.info(`Testing model ${modelId}...`);

      const response = await fetch(`${this.serverUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(testRequest),
      });

      if (!response.ok) {
        throw new Error(`Model test failed with status: ${response.status}`);
      }

      const result = (await response.json()) as OllamaResponse;

      if (!result.response) {
        throw new Error('No valid response content received');
      }

      logger.info(`Model ${modelId} test response:`, {
        content: result.response,
        model: result.model,
        timestamp: new Date().toISOString(),
      });

      return true;
    } catch (error) {
      logger.error(`Model ${modelId} test failed:`, {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined,
        timestamp: new Date().toISOString(),
      });
      return false;
    }
  }

  /**
   * Asynchronously tests the configured text models to ensure they are working properly.
   * Logs the test results for each model and outputs a warning if any models fail the test.
   * @returns {Promise<void>} A Promise that resolves when all configured models have been tested.
   */
  private async testTextModels(): Promise<void> {
    logger.info('Testing configured text models...');

    const results = await Promise.all([
      this.testModel(this.configuredModels.small),
      this.testModel(this.configuredModels.medium),
    ]);

    const [smallWorking, mediumWorking] = results;

    if (!smallWorking || !mediumWorking) {
      const failedModels = [];
      if (!smallWorking) failedModels.push('small');
      if (!mediumWorking) failedModels.push('medium');

      logger.warn('Some models failed the test:', {
        failedModels,
        small: this.configuredModels.small,
        medium: this.configuredModels.medium,
      });
    } else {
      logger.success('All configured models passed the test');
    }
  }

  /**
   * Asynchronously initializes the Ollama service by checking server status,
   * fetching available models, and testing text models.
   *
   * @returns A Promise that resolves when initialization is complete
   */
  public async initialize(): Promise<void> {
    try {
      if (this.initialized) {
        logger.info('Ollama already initialized, skipping initialization');
        return;
      }

      logger.info('Starting Ollama initialization...');
      const serverAvailable = await this.checkServerStatus();

      if (!serverAvailable) {
        throw new Error('Ollama server is not available');
      }

      await this.fetchAvailableModels();
      await this.testTextModels();

      this.initialized = true;
      logger.success('Ollama initialization complete');
    } catch (error) {
      logger.error('Ollama initialization failed:', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined,
      });
      throw error;
    }
  }

  /**
   * Retrieves the available Ollama models.
   *
   * @returns {OllamaModel[]} An array of OllamaModel objects representing the available models.
   */
  public getAvailableModels(): OllamaModel[] {
    return this.availableModels;
  }

  /**
   * Check if the object is initialized.
   * @returns {boolean} True if the object is initialized, false otherwise.
   */
  public isInitialized(): boolean {
    return this.initialized;
  }

  /**
   * Generates text using the Ollama AI model.
   *
   * @param {GenerateTextParams} params - The parameters for generating text.
   * @param {boolean} [isInitialized=false] - Flag indicating if Ollama is already initialized.
   * @returns {Promise<string>} - A promise that resolves with the generated text.
   */
  public async generateText(params: GenerateTextParams, isInitialized = false): Promise<string> {
    try {
      // Log entry point with all parameters
      logger.info('Ollama generateText entry:', {
        isInitialized,
        currentInitState: this.initialized,
        managerInitState: this.isInitialized(),
        modelType: params.modelType,
        contextLength: params.prompt?.length,
        timestamp: new Date().toISOString(),
      });

      // Only initialize if not already initialized and not marked as initialized
      if (!this.initialized && !isInitialized) {
        throw new Error('Ollama not initialized. Please initialize before generating text.');
      }

      logger.info('Ollama preparing request:', {
        model:
          params.modelType === ModelType.TEXT_LARGE
            ? this.configuredModels.medium
            : this.configuredModels.small,
        contextLength: params.prompt.length,
        timestamp: new Date().toISOString(),
      });

      const request = {
        model:
          params.modelType === ModelType.TEXT_LARGE
            ? this.configuredModels.medium
            : this.configuredModels.small,
        prompt: params.prompt,
        stream: false,
        options: {
          temperature: 0.7,
          top_p: 0.9,
          num_predict: 8192,
          repeat_penalty: 1.2,
          frequency_penalty: 0.7,
          presence_penalty: 0.7,
        },
      };

      const response = await fetch(`${this.serverUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
      });

      if (!response.ok) {
        throw new Error(`Ollama request failed: ${response.status}`);
      }

      const result = (await response.json()) as OllamaResponse;

      if (!result.response) {
        throw new Error('No valid response content received from Ollama');
      }

      let responseText = result.response;

      // Log raw response for debugging
      logger.info('Raw response structure:', {
        responseLength: responseText.length,
        hasAction: responseText.includes('action'),
        hasThinkTag: responseText.includes('<think>'),
      });

      // Clean think tags if present
      if (responseText.includes('<think>')) {
        logger.info('Cleaning think tags from response');
        responseText = responseText.replace(/<think>[\s\S]*?<\/think>\n?/g, '');
        logger.info('Think tags removed from response');
      }

      logger.info('Ollama request completed successfully:', {
        responseLength: responseText.length,
        hasThinkTags: responseText.includes('<think>'),
        timestamp: new Date().toISOString(),
      });

      return responseText;
    } catch (error) {
      logger.error('Ollama text generation error:', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined,
        phase: 'text generation',
        timestamp: new Date().toISOString(),
      });
      throw error;
    }
  }
}
